# ALIFE 2022 Student Abstract & Essay Winner
by [Federico Pigozzi](https://twitter.com/PigozziFederico)

Large Language Models (LLMs) (Devlin et al., 2018) have propelled breakthroughs in sequence modeling by pre-training on large web corpora, to the point of seemingly incorporating “universal” knowledge (Lu et al., 2021). But, to what extent is such knowledge universal remains an open question. In other words, do LLMs ground knowledge that is so universal to solve far-fetched downstream tasks?
Most notably, Reinforcement Learning (RL) agents lag behind the computational abilities of animals and they might benefit from the common sense embedded in LLMs as if those were world models (Ha and Schmidhuber, 2018).
In particular, we wonder whether LLMs can be used as off-the-shelf reservoirs for RL tasks by simply optimizing a readout function on top of them.
Proving that LLMs ground knowledge that is so universal to solve RL tasks would be a relevant discovery. As a consequence, being the LLM reservoir frozen, we can solve tasks by optimizing much fewer parameters than usual.

//View the full submission and the other Student Abstract and Essay submissions [here](https://www.2022.alife.org/_files/ugd/534077_2d1b5f057cae4d8abdedf7774aec177d.pdf).
